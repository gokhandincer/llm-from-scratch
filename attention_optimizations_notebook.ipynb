{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Attention Optimizasyonlarƒ±: Flash Attention'dan MLA'ya\n",
    "\n",
    "Bu notebook'ta farklƒ± attention mekanizmalarƒ±nƒ± implement edip kar≈üƒ±la≈ütƒ±racaƒüƒ±z:\n",
    "\n",
    "1. Standard Multi-Head Attention (MHA)\n",
    "2. Flash Attention (PyTorch native)\n",
    "3. Multi-Query Attention (MQA)\n",
    "4. Grouped-Query Attention (GQA)\n",
    "5. Multi-head Latent Attention (MLA)\n",
    "\n",
    "## ƒ∞√ßindekiler\n",
    "\n",
    "1. [Kurulum](#1-kurulum)\n",
    "2. [Standard MHA](#2-standard-mha)\n",
    "3. [Flash Attention](#3-flash-attention)\n",
    "4. [Multi-Query Attention](#4-multi-query-attention)\n",
    "5. [Grouped-Query Attention](#5-grouped-query-attention)\n",
    "6. [Multi-head Latent Attention](#6-multi-head-latent-attention)\n",
    "7. [Benchmark ve Kar≈üƒ±la≈ütƒ±rma](#7-benchmark-ve-kar≈üƒ±la≈ütƒ±rma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Kurulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Device: {device}\")\n",
    "print(f\"üì¶ PyTorch: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AttentionConfig:\n",
    "    \"\"\"Attention konfig√ºrasyonu\"\"\"\n",
    "    d_model: int = 512\n",
    "    num_heads: int = 8\n",
    "    num_kv_groups: int = 2      # GQA i√ßin\n",
    "    d_latent: int = 128          # MLA i√ßin\n",
    "    dropout: float = 0.1\n",
    "    max_seq_len: int = 2048\n",
    "    \n",
    "config = AttentionConfig()\n",
    "print(f\"‚öôÔ∏è Config: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Standard MHA\n",
    "\n",
    "Baseline olarak standart Multi-Head Attention implementasyonu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardMHA(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Multi-Head Attention\n",
    "    \n",
    "    Her head i√ßin ayrƒ± Q, K, V.\n",
    "    KV-Cache boyutu: 2 √ó num_heads √ó seq_len √ó d_head\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: AttentionConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads = config.num_heads\n",
    "        self.d_head = config.d_model // config.num_heads\n",
    "        self.d_model = config.d_model\n",
    "        \n",
    "        # Q, K, V projections\n",
    "        self.W_q = nn.Linear(config.d_model, config.d_model)\n",
    "        self.W_k = nn.Linear(config.d_model, config.d_model)\n",
    "        self.W_v = nn.Linear(config.d_model, config.d_model)\n",
    "        self.W_o = nn.Linear(config.d_model, config.d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Causal mask\n",
    "        mask = torch.triu(torch.ones(config.max_seq_len, config.max_seq_len), diagonal=1).bool()\n",
    "        self.register_buffer('causal_mask', mask)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor,\n",
    "        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, d_model]\n",
    "            kv_cache: Tuple of (K_cache, V_cache) for inference\n",
    "            use_cache: Whether to return new KV cache\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Projections\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Reshape: [B, T, num_heads, d_head] -> [B, num_heads, T, d_head]\n",
    "        Q = Q.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        K = K.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        V = V.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # KV Cache handling\n",
    "        if kv_cache is not None:\n",
    "            K_cache, V_cache = kv_cache\n",
    "            K = torch.cat([K_cache, K], dim=2)\n",
    "            V = torch.cat([V_cache, V], dim=2)\n",
    "        \n",
    "        new_cache = (K, V) if use_cache else None\n",
    "        \n",
    "        # Attention\n",
    "        T_full = K.size(2)\n",
    "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        \n",
    "        # Causal mask (sadece yeni tokenler i√ßin)\n",
    "        if kv_cache is None:\n",
    "            scores = scores.masked_fill(self.causal_mask[:T, :T], float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Output\n",
    "        out = attn @ V\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.W_o(out)\n",
    "        \n",
    "        return out, new_cache\n",
    "    \n",
    "    def get_kv_cache_size(self, seq_len: int, batch_size: int = 1) -> int:\n",
    "        \"\"\"KV-Cache boyutunu hesapla (byte cinsinden)\"\"\"\n",
    "        # 2 (K+V) √ó batch √ó num_heads √ó seq_len √ó d_head √ó 4 (float32)\n",
    "        return 2 * batch_size * self.num_heads * seq_len * self.d_head * 4\n",
    "\n",
    "# Test\n",
    "mha = StandardMHA(config).to(device)\n",
    "x = torch.randn(2, 64, config.d_model).to(device)\n",
    "out, cache = mha(x, use_cache=True)\n",
    "\n",
    "print(f\"‚úÖ Standard MHA\")\n",
    "print(f\"   Input:  {x.shape}\")\n",
    "print(f\"   Output: {out.shape}\")\n",
    "print(f\"   KV Cache: K={cache[0].shape}, V={cache[1].shape}\")\n",
    "print(f\"   Cache size: {mha.get_kv_cache_size(64) / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Flash Attention\n",
    "\n",
    "PyTorch 2.0+ ile gelen `scaled_dot_product_attention` otomatik olarak Flash Attention kullanƒ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashMHA(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention with Flash Attention\n",
    "    \n",
    "    PyTorch'un native scaled_dot_product_attention kullanƒ±r.\n",
    "    Otomatik olarak Flash Attention'a d√º≈üer (CUDA varsa).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: AttentionConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads = config.num_heads\n",
    "        self.d_head = config.d_model // config.num_heads\n",
    "        self.d_model = config.d_model\n",
    "        \n",
    "        self.W_q = nn.Linear(config.d_model, config.d_model)\n",
    "        self.W_k = nn.Linear(config.d_model, config.d_model)\n",
    "        self.W_v = nn.Linear(config.d_model, config.d_model)\n",
    "        self.W_o = nn.Linear(config.d_model, config.d_model)\n",
    "        \n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor,\n",
    "        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
    "        \n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        Q = self.W_q(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        if kv_cache is not None:\n",
    "            K = torch.cat([kv_cache[0], K], dim=2)\n",
    "            V = torch.cat([kv_cache[1], V], dim=2)\n",
    "        \n",
    "        new_cache = (K, V) if use_cache else None\n",
    "        \n",
    "        # üöÄ Flash Attention via PyTorch native SDPA\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            Q, K, V,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.dropout if self.training else 0.0,\n",
    "            is_causal=(kv_cache is None)  # Causal sadece ilk forward'da\n",
    "        )\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.W_o(out)\n",
    "        \n",
    "        return out, new_cache\n",
    "    \n",
    "    def get_kv_cache_size(self, seq_len: int, batch_size: int = 1) -> int:\n",
    "        return 2 * batch_size * self.num_heads * seq_len * self.d_head * 4\n",
    "\n",
    "# Test\n",
    "flash_mha = FlashMHA(config).to(device)\n",
    "out, cache = flash_mha(x, use_cache=True)\n",
    "\n",
    "print(f\"‚úÖ Flash MHA\")\n",
    "print(f\"   Output: {out.shape}\")\n",
    "\n",
    "# Flash Attention backend kontrol√º\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüîç SDPA Backend Kontrol√º:\")\n",
    "    print(f\"   Flash: {torch.backends.cuda.flash_sdp_enabled()}\")\n",
    "    print(f\"   Memory Efficient: {torch.backends.cuda.mem_efficient_sdp_enabled()}\")\n",
    "    print(f\"   Math: {torch.backends.cuda.math_sdp_enabled()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Multi-Query Attention (MQA)\n",
    "\n",
    "T√ºm head'ler i√ßin **tek K ve V** kullanƒ±r. KV-Cache'i dramatik ≈üekilde k√º√ß√ºlt√ºr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Query Attention (MQA)\n",
    "    \n",
    "    - Q: Her head i√ßin ayrƒ± (num_heads adet)\n",
    "    - K, V: T√ºm head'ler i√ßin tek (payla≈üƒ±mlƒ±)\n",
    "    \n",
    "    KV-Cache boyutu: 2 √ó seq_len √ó d_head (num_heads ile √ßarpƒ±lmaz!)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: AttentionConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads = config.num_heads\n",
    "        self.d_head = config.d_model // config.num_heads\n",
    "        self.d_model = config.d_model\n",
    "        \n",
    "        # Q: Full projection (t√ºm head'ler)\n",
    "        self.W_q = nn.Linear(config.d_model, config.d_model)\n",
    "        \n",
    "        # K, V: Sadece tek head boyutunda!\n",
    "        self.W_k = nn.Linear(config.d_model, self.d_head)\n",
    "        self.W_v = nn.Linear(config.d_model, self.d_head)\n",
    "        \n",
    "        self.W_o = nn.Linear(config.d_model, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Causal mask\n",
    "        mask = torch.triu(torch.ones(config.max_seq_len, config.max_seq_len), diagonal=1).bool()\n",
    "        self.register_buffer('causal_mask', mask)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor,\n",
    "        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
    "        \n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Q: [B, num_heads, T, d_head]\n",
    "        Q = self.W_q(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # K, V: [B, T, d_head] (tek head!)\n",
    "        K = self.W_k(x)  # [B, T, d_head]\n",
    "        V = self.W_v(x)  # [B, T, d_head]\n",
    "        \n",
    "        # KV Cache\n",
    "        if kv_cache is not None:\n",
    "            K = torch.cat([kv_cache[0], K], dim=1)\n",
    "            V = torch.cat([kv_cache[1], V], dim=1)\n",
    "        \n",
    "        new_cache = (K, V) if use_cache else None\n",
    "        \n",
    "        # K, V'yi t√ºm head'lere broadcast et\n",
    "        # [B, T, d_head] -> [B, 1, T, d_head] -> [B, num_heads, T, d_head]\n",
    "        T_full = K.size(1)\n",
    "        K = K.unsqueeze(1).expand(-1, self.num_heads, -1, -1)  # Broadcast\n",
    "        V = V.unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n",
    "        \n",
    "        # Attention\n",
    "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        \n",
    "        if kv_cache is None:\n",
    "            scores = scores.masked_fill(self.causal_mask[:T, :T_full], float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = attn @ V\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.W_o(out)\n",
    "        \n",
    "        return out, new_cache\n",
    "    \n",
    "    def get_kv_cache_size(self, seq_len: int, batch_size: int = 1) -> int:\n",
    "        \"\"\"MQA: Sadece 1 head i√ßin cache (num_heads ile √ßarpƒ±lmaz!)\"\"\"\n",
    "        return 2 * batch_size * seq_len * self.d_head * 4\n",
    "\n",
    "# Test\n",
    "mqa = MultiQueryAttention(config).to(device)\n",
    "out, cache = mqa(x, use_cache=True)\n",
    "\n",
    "print(f\"‚úÖ Multi-Query Attention (MQA)\")\n",
    "print(f\"   Output: {out.shape}\")\n",
    "print(f\"   KV Cache: K={cache[0].shape}, V={cache[1].shape}\")\n",
    "print(f\"   Cache size: {mqa.get_kv_cache_size(64) / 1024:.1f} KB\")\n",
    "print(f\"\\n   üìâ MHA'ya g√∂re cache azalma: {config.num_heads}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Grouped-Query Attention (GQA)\n",
    "\n",
    "MHA ve MQA arasƒ± denge. K,V i√ßin **birka√ß grup** kullanƒ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped-Query Attention (GQA)\n",
    "    \n",
    "    - Q: Her head i√ßin ayrƒ± (num_heads adet)\n",
    "    - K, V: Grup ba≈üƒ±na bir tane (num_kv_groups adet)\n",
    "    \n",
    "    √ñrnek: 8 Q head, 2 KV group\n",
    "    - Head 0,1,2,3 ‚Üí KV Group 0\n",
    "    - Head 4,5,6,7 ‚Üí KV Group 1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: AttentionConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert config.num_heads % config.num_kv_groups == 0, \\\n",
    "            \"num_heads must be divisible by num_kv_groups\"\n",
    "        \n",
    "        self.num_heads = config.num_heads\n",
    "        self.num_kv_groups = config.num_kv_groups\n",
    "        self.heads_per_group = config.num_heads // config.num_kv_groups\n",
    "        self.d_head = config.d_model // config.num_heads\n",
    "        self.d_model = config.d_model\n",
    "        \n",
    "        # Q: T√ºm head'ler i√ßin\n",
    "        self.W_q = nn.Linear(config.d_model, config.d_model)\n",
    "        \n",
    "        # K, V: Sadece grup sayƒ±sƒ± kadar\n",
    "        kv_dim = self.num_kv_groups * self.d_head\n",
    "        self.W_k = nn.Linear(config.d_model, kv_dim)\n",
    "        self.W_v = nn.Linear(config.d_model, kv_dim)\n",
    "        \n",
    "        self.W_o = nn.Linear(config.d_model, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Causal mask\n",
    "        mask = torch.triu(torch.ones(config.max_seq_len, config.max_seq_len), diagonal=1).bool()\n",
    "        self.register_buffer('causal_mask', mask)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor,\n",
    "        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
    "        \n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Q: [B, num_heads, T, d_head]\n",
    "        Q = self.W_q(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # K, V: [B, T, num_kv_groups, d_head]\n",
    "        K = self.W_k(x).view(B, T, self.num_kv_groups, self.d_head)\n",
    "        V = self.W_v(x).view(B, T, self.num_kv_groups, self.d_head)\n",
    "        \n",
    "        # KV Cache (grup formatƒ±nda)\n",
    "        if kv_cache is not None:\n",
    "            K = torch.cat([kv_cache[0], K], dim=1)\n",
    "            V = torch.cat([kv_cache[1], V], dim=1)\n",
    "        \n",
    "        new_cache = (K, V) if use_cache else None\n",
    "        \n",
    "        # K, V'yi head'lere expand et\n",
    "        # [B, T, num_kv_groups, d_head] -> [B, T, num_heads, d_head]\n",
    "        K = K.repeat_interleave(self.heads_per_group, dim=2)\n",
    "        V = V.repeat_interleave(self.heads_per_group, dim=2)\n",
    "        \n",
    "        # Transpose: [B, num_heads, T, d_head]\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        T_full = K.size(2)\n",
    "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        \n",
    "        if kv_cache is None:\n",
    "            scores = scores.masked_fill(self.causal_mask[:T, :T_full], float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = attn @ V\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.W_o(out)\n",
    "        \n",
    "        return out, new_cache\n",
    "    \n",
    "    def get_kv_cache_size(self, seq_len: int, batch_size: int = 1) -> int:\n",
    "        \"\"\"GQA: num_kv_groups kadar K,V\"\"\"\n",
    "        return 2 * batch_size * self.num_kv_groups * seq_len * self.d_head * 4\n",
    "\n",
    "# Test\n",
    "gqa = GroupedQueryAttention(config).to(device)\n",
    "out, cache = gqa(x, use_cache=True)\n",
    "\n",
    "print(f\"‚úÖ Grouped-Query Attention (GQA)\")\n",
    "print(f\"   num_heads: {config.num_heads}, num_kv_groups: {config.num_kv_groups}\")\n",
    "print(f\"   Output: {out.shape}\")\n",
    "print(f\"   KV Cache: K={cache[0].shape}, V={cache[1].shape}\")\n",
    "print(f\"   Cache size: {gqa.get_kv_cache_size(64) / 1024:.1f} KB\")\n",
    "print(f\"\\n   üìâ MHA'ya g√∂re cache azalma: {config.num_heads // config.num_kv_groups}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Multi-head Latent Attention (MLA)\n",
    "\n",
    "DeepSeek'in yeniliƒüi: K,V'yi d√º≈ü√ºk boyutlu latent space'e sƒ±kƒ±≈ütƒ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadLatentAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head Latent Attention (MLA)\n",
    "    \n",
    "    DeepSeek-V2/V3'√ºn yakla≈üƒ±mƒ±:\n",
    "    - K, V d√º≈ü√ºk boyutlu latent vekt√∂re sƒ±kƒ±≈ütƒ±rƒ±lƒ±r\n",
    "    - Cache sadece latent'i saklar\n",
    "    - Inference'da latent ‚Üí K, V decompress edilir\n",
    "    \n",
    "    Compression ratio: d_model / d_latent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: AttentionConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads = config.num_heads\n",
    "        self.d_head = config.d_model // config.num_heads\n",
    "        self.d_model = config.d_model\n",
    "        self.d_latent = config.d_latent\n",
    "        \n",
    "        # Q projection (standart)\n",
    "        self.W_q = nn.Linear(config.d_model, config.d_model)\n",
    "        \n",
    "        # KV compression (down-projection)\n",
    "        self.W_kv_down = nn.Linear(config.d_model, config.d_latent)\n",
    "        \n",
    "        # KV decompression (up-projection)\n",
    "        self.W_k_up = nn.Linear(config.d_latent, config.d_model)\n",
    "        self.W_v_up = nn.Linear(config.d_latent, config.d_model)\n",
    "        \n",
    "        self.W_o = nn.Linear(config.d_model, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Causal mask\n",
    "        mask = torch.triu(torch.ones(config.max_seq_len, config.max_seq_len), diagonal=1).bool()\n",
    "        self.register_buffer('causal_mask', mask)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor,\n",
    "        kv_cache: Optional[torch.Tensor] = None,  # Sadece latent!\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, d_model]\n",
    "            kv_cache: Latent cache [batch, cache_len, d_latent]\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Q: [B, num_heads, T, d_head]\n",
    "        Q = self.W_q(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # KV: Compress to latent\n",
    "        kv_latent = self.W_kv_down(x)  # [B, T, d_latent]\n",
    "        \n",
    "        # Cache: Sadece latent saklanƒ±r!\n",
    "        if kv_cache is not None:\n",
    "            kv_latent_full = torch.cat([kv_cache, kv_latent], dim=1)\n",
    "        else:\n",
    "            kv_latent_full = kv_latent\n",
    "        \n",
    "        new_cache = kv_latent_full if use_cache else None\n",
    "        \n",
    "        # Decompress: latent ‚Üí K, V\n",
    "        K = self.W_k_up(kv_latent_full)  # [B, T_full, d_model]\n",
    "        V = self.W_v_up(kv_latent_full)  # [B, T_full, d_model]\n",
    "        \n",
    "        # Reshape to heads\n",
    "        T_full = K.size(1)\n",
    "        K = K.view(B, T_full, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        V = V.view(B, T_full, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        \n",
    "        if kv_cache is None:\n",
    "            scores = scores.masked_fill(self.causal_mask[:T, :T_full], float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = attn @ V\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.W_o(out)\n",
    "        \n",
    "        return out, new_cache\n",
    "    \n",
    "    def get_kv_cache_size(self, seq_len: int, batch_size: int = 1) -> int:\n",
    "        \"\"\"MLA: Sadece latent saklanƒ±r!\"\"\"\n",
    "        return batch_size * seq_len * self.d_latent * 4\n",
    "\n",
    "# Test\n",
    "mla = MultiheadLatentAttention(config).to(device)\n",
    "out, cache = mla(x, use_cache=True)\n",
    "\n",
    "print(f\"‚úÖ Multi-head Latent Attention (MLA)\")\n",
    "print(f\"   d_latent: {config.d_latent}\")\n",
    "print(f\"   Output: {out.shape}\")\n",
    "print(f\"   Latent Cache: {cache.shape}\")\n",
    "print(f\"   Cache size: {mla.get_kv_cache_size(64) / 1024:.1f} KB\")\n",
    "\n",
    "compression = (2 * config.num_heads * config.d_model // config.num_heads) / config.d_latent\n",
    "print(f\"\\n   üìâ MHA'ya g√∂re cache azalma: {compression:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Benchmark ve Kar≈üƒ±la≈ütƒ±rma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_attention(model, x, num_runs=100, warmup=10):\n",
    "    \"\"\"Attention mod√ºl√ºn√ºn hƒ±zƒ±nƒ± √∂l√ß\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(x)\n",
    "    \n",
    "    # Sync (GPU i√ßin)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model(x)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    elapsed = time.perf_counter() - start\n",
    "    return elapsed / num_runs * 1000  # ms\n",
    "\n",
    "# Modelleri hazƒ±rla\n",
    "models = {\n",
    "    'Standard MHA': StandardMHA(config).to(device),\n",
    "    'Flash MHA': FlashMHA(config).to(device),\n",
    "    'MQA': MultiQueryAttention(config).to(device),\n",
    "    'GQA': GroupedQueryAttention(config).to(device),\n",
    "    'MLA': MultiheadLatentAttention(config).to(device),\n",
    "}\n",
    "\n",
    "# Farklƒ± sequence uzunluklarƒ± i√ßin benchmark\n",
    "seq_lengths = [64, 128, 256, 512, 1024]\n",
    "results = {name: [] for name in models}\n",
    "\n",
    "print(\"‚è±Ô∏è Benchmark ba≈ülƒ±yor...\\n\")\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    x_bench = torch.randn(4, seq_len, config.d_model).to(device)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            time_ms = benchmark_attention(model, x_bench, num_runs=50)\n",
    "            results[name].append(time_ms)\n",
    "        except Exception as e:\n",
    "            results[name].append(float('nan'))\n",
    "            print(f\"‚ö†Ô∏è {name} failed at seq_len={seq_len}: {e}\")\n",
    "    \n",
    "    print(f\"Seq length {seq_len}: Done\")\n",
    "\n",
    "print(\"\\n‚úÖ Benchmark tamamlandƒ±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sonu√ßlarƒ± g√∂rselle≈ütir\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sol: Execution time\n",
    "for name, times in results.items():\n",
    "    axes[0].plot(seq_lengths, times, marker='o', label=name, linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('Sequence Length')\n",
    "axes[0].set_ylabel('Time (ms)')\n",
    "axes[0].set_title('Attention Forward Pass Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Saƒü: KV-Cache boyutu\n",
    "cache_sizes = {\n",
    "    'Standard MHA': [models['Standard MHA'].get_kv_cache_size(s) / 1024 for s in seq_lengths],\n",
    "    'MQA': [models['MQA'].get_kv_cache_size(s) / 1024 for s in seq_lengths],\n",
    "    'GQA': [models['GQA'].get_kv_cache_size(s) / 1024 for s in seq_lengths],\n",
    "    'MLA': [models['MLA'].get_kv_cache_size(s) / 1024 for s in seq_lengths],\n",
    "}\n",
    "\n",
    "x_pos = np.arange(len(seq_lengths))\n",
    "width = 0.2\n",
    "\n",
    "for i, (name, sizes) in enumerate(cache_sizes.items()):\n",
    "    axes[1].bar(x_pos + i*width, sizes, width, label=name)\n",
    "\n",
    "axes[1].set_xlabel('Sequence Length')\n",
    "axes[1].set_ylabel('KV-Cache Size (KB)')\n",
    "axes[1].set_title('KV-Cache Memory Usage')\n",
    "axes[1].set_xticks(x_pos + width*1.5)\n",
    "axes[1].set_xticklabels(seq_lengths)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √ñzet tablo\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä √ñZET KAR≈ûILA≈ûTIRMA (seq_len=512)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "seq_idx = seq_lengths.index(512)\n",
    "base_time = results['Standard MHA'][seq_idx]\n",
    "base_cache = models['Standard MHA'].get_kv_cache_size(512)\n",
    "\n",
    "print(f\"\\n{'Model':<20} {'Time (ms)':<12} {'Speedup':<10} {'Cache (KB)':<12} {'Cache Reduction':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    time_ms = results[name][seq_idx]\n",
    "    speedup = base_time / time_ms if time_ms > 0 else 0\n",
    "    \n",
    "    if name == 'Flash MHA':\n",
    "        cache_kb = base_cache / 1024\n",
    "        reduction = 1.0\n",
    "    else:\n",
    "        cache_kb = model.get_kv_cache_size(512) / 1024\n",
    "        reduction = base_cache / (cache_kb * 1024)\n",
    "    \n",
    "    print(f\"{name:<20} {time_ms:<12.2f} {speedup:<10.2f}x {cache_kb:<12.1f} {reduction:<15.1f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö √ñzet\n",
    "\n",
    "| Teknik | Ana Fikir | KV-Cache | Kullanƒ±m |\n",
    "|--------|-----------|----------|----------|\n",
    "| **Standard MHA** | Baseline | 1x | Ara≈ütƒ±rma |\n",
    "| **Flash Attention** | IO-aware tiling | 1x | Her yerde! |\n",
    "| **MQA** | Tek K,V | 1/num_heads | Edge/Mobile |\n",
    "| **GQA** | Gruplu K,V | 1/num_groups | LLaMA, Mistral |\n",
    "| **MLA** | Latent compression | 1/16+ | DeepSeek |\n",
    "\n",
    "### √ñneriler\n",
    "\n",
    "- **Eƒüitim:** Flash Attention + MHA\n",
    "- **Inference (kƒ±sa context):** GQA\n",
    "- **Inference (uzun context):** MLA\n",
    "- **D√º≈ü√ºk kaynak:** MQA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
